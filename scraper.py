import schedule
import time
from datetime import datetime, timezone, timedelta
import csv
import os
import requests
from bs4 import BeautifulSoup
import re

DATA_FILE = "investing_oil.csv"

# Zmienne Å›rodowiskowe (Railway)
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")

# Walidacja Supabase
if not SUPABASE_URL or not SUPABASE_KEY:
    print("âš ï¸  SUPABASE_URL lub SUPABASE_KEY nie jest ustawiony!")

def save_to_csv(data):
    file_exists = os.path.isfile(DATA_FILE)
    try:
        with open(DATA_FILE, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=["timestamp", "volume"])
            if not file_exists:
                writer.writeheader()
            writer.writerow(data)
        print(f"âœ… Dane zapisane do {DATA_FILE}")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d przy zapisywaniu: {e}")

def send_to_webhook(data):
    """WysyÅ‚a dane do Supabase"""
    try:
        headers = {
            "apikey": SUPABASE_KEY,
            "Content-Type": "application/json",
            "Prefer": "return=minimal"
        }
        
        payload = {
            "timestamp": data["timestamp"],
            "volume": data["volume"]
        }
        
        url = f"{SUPABASE_URL}/rest/v1/investing_oil"
        response = requests.post(url, json=payload, headers=headers, timeout=10)
        
        if response.status_code in [200, 201]:
            print(f"âœ… Dane wysÅ‚ane do Supabase: {response.status_code}")
        else:
            print(f"âš ï¸ Supabase zwrÃ³ciÅ‚: {response.status_code}")
            print(f"   OdpowiedÅº: {response.text}")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d Supabase: {e}")

def scrape_investing_volume():
    """Scrapeuje wolumen ropy z Investing.com uÅ¼ywajÄ…c BeautifulSoup"""
    try:
        print("  ğŸŒ Pobieranie strony...")
        
        # Headers aby nie zostaÄ‡ zablokowanym
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        # Pobierz stronÄ™
        response = requests.get(
            "https://pl.investing.com/commodities/crude-oil",
            headers=headers,
            timeout=15
        )
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Szukaj elementu data-test="volume"
        volume_element = soup.find('dd', {'data-test': 'volume'})
        
        if volume_element:
            # WyciÄ…gnij tekst
            volume_text = volume_element.get_text()
            print(f"  ğŸ“ Znaleziony tekst: {volume_text}")
            
            # WyciÄ…gnij liczbÄ™
            match = re.search(r'[\d]+[\.,][\d]+', volume_text)
            if match:
                volume = match.group(0).replace(",", ".")
                print(f"  âœ… WyodrÄ™bniony wolumen: {volume}")
                return volume
        
        print("  âš ï¸  Element data-test='volume' nie znaleziony")
        return None
            
    except Exception as e:
        print(f"âš ï¸  BÅ‚Ä…d przy scrapeowaniu: {e}")
        return None

def scrape_investing_data():
    """
    Scrapuje wolumen ropy z Investing.com przy uÅ¼yciu BeautifulSoup.
    Zbiera TYLKO dane ze strony - bez mock danych!
    """
    # Sprawdzenie czy jesteÅ›my w sesji handlowej ropy
    if not is_oil_trading_session():
        print(f"â¸ï¸  Poza sesjÄ… handlowÄ… ropy (UTC: pon-piÄ… 14:00-19:30)")
        return
    
    try:
        print(f"ğŸ”„ Scrapowanie Investing.com ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})...")
        
        # Scrapeuj stronÄ™
        volume = scrape_investing_volume()
        
        if not volume:
            print(f"  âš ï¸  Nie udaÅ‚o siÄ™ pobraÄ‡ wolumenu ze strony")
            return
        
        print(f"  ğŸ“Š Wolumen (ze strony): {volume}")
        print("-" * 50)
        
        data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "volume": volume
        }
        
        save_to_csv(data)
        send_to_webhook(data)
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d: {e}")
        print("-" * 50)

def is_oil_trading_session():
    """
    Sprawdza czy jesteÅ›my w sesji handlowej ropy.
    Sesja: poniedziaÅ‚ek-piÄ…tek 9:00-14:30 UTC-5 (EST)
    
    Konwersja: UTC-5 to UTC+7 to +12h razem
    9:00 UTC-5 = 21:00 UTC-5 poprzedniego dnia
    Ale Å‚atwiej: po prostu codziennie od 14:00 UTC do 19:30 UTC
    (bo 9:00 UTC-5 = 14:00 UTC, 14:30 UTC-5 = 19:30 UTC)
    """
    # Pobieramy aktualny czas w UTC
    now_utc = datetime.now(timezone.utc)
    
    weekday_utc = now_utc.weekday()  # 0=pon, 4=piÄ…, 5=sob
    hour_utc = now_utc.hour
    minute_utc = now_utc.minute
    
    # Sesja: pon-piÄ… (0-4) od 14:00 do 19:30 UTC
    is_weekday = 0 <= weekday_utc <= 4
    is_trading_time = (hour_utc >= 14 and hour_utc < 19) or \
                      (hour_utc == 19 and minute_utc <= 30)
    
    return is_weekday and is_trading_time

def job():
    scrape_investing_data()

if __name__ == "__main__":
    print("ğŸš€ SCRAPER INVESTING.COM URUCHOMIONY!")
    print(f"   Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("   Å¹rÃ³dÅ‚o: https://pl.investing.com/commodities/crude-oil")
    print("   Zbieranie: co 3 minuty (TEST MODE)")
    print("   Sesja: poniedziaÅ‚ek-piÄ…tek, UTC: 14:00-19:30")
    print("   Tryb: LIVE (zbieranie TYLKO ze strony - BeautifulSoup)")
    print(f"   SUPABASE: {'âœ… Configured' if SUPABASE_URL and SUPABASE_KEY else 'âŒ Not configured'}")
    print("="*50)
    
    # Uruchom zbieranie OD RAZU
    print("ğŸ“¥ Zbieranie danych na starcie...")
    job()
    print("="*50)
    
    # TEST: Co 3 minuty zamiast :00 i :30
    schedule.every(3).minutes.do(job)
    
    while True:
        schedule.run_pending()
        time.sleep(60)
