# ğŸ“Š DOKUMENTACJA SCRAPERA - Investing.com Oil Volume

## ğŸ¯ Cel Projektu
Scraper zbiera dane o wolumenie ropy naftowej z serwisu Investing.com (`https://pl.investing.com/commodities/crude-oil`) co **30 minut** (o peÅ‚nych godzinach :00 i :30) i zapisuje je do bazy danych Supabase oraz lokalnego pliku CSV.

---

## ğŸ—ï¸ Architektura Systemu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAILWAY.APP (Cloud)                  â”‚
â”‚  - Hosting: 24/7 bez przerw                             â”‚
â”‚  - Python 3.11                                          â”‚
â”‚  - Schedule library (job scheduler)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚
       â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase DB   â”‚  â”‚  CSV File        â”‚
â”‚  (investing_oil)â”‚  â”‚ (investing_oil.csv)
â”‚                â”‚  â”‚                  â”‚
â”‚ - timestamp    â”‚  â”‚ - timestamp      â”‚
â”‚ - volume       â”‚  â”‚ - volume         â”‚
â”‚ - created_at   â”‚  â”‚ - created_at     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Struktura Projektu

```
crawl4ai-scraper/
â”œâ”€â”€ scraper.py              # GÅ‚Ã³wny skrypt scrapera
â”œâ”€â”€ requirements.txt        # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ Procfile               # Konfiguracja Railway
â”œâ”€â”€ .git/                  # Git repository
â”œâ”€â”€ README.md              # Instrukcje
â””â”€â”€ DOKUMENTACJA.md        # Ta dokumentacja
```

---

## âš™ï¸ Technologia

| Komponenta | Wersja | Rola |
|-----------|--------|------|
| **Python** | 3.11 | JÄ™zyk programowania |
| **Railway** | - | Hosting (24/7) |
| **Supabase** | - | Baza danych PostgreSQL |
| **Schedule** | 3.10+ | Job scheduler (every().hour.at()) |
| **Requests** | 2.28+ | HTTP requests |
| **Git** | - | Version control (GitHub) |

---

## ğŸ”§ Konfiguracja

### Zmienne Åšrodowiskowe (Railway)

W Railway â†’ Variables dodaj:

```
SUPABASE_URL = https://your-project.supabase.co
SUPABASE_KEY = your-anon-key-here
MOCK_VOLUME = 77.626
```

â„¹ï¸ Nie commituj wraÅ¼liwych danych! UÅ¼yj `.env` lokalnie i zmiennych Å›rodowiskowych na Railway.

### Mock Data (Lokalna Konfiguracja)

Pobierz aktualnÄ… wartoÅ›Ä‡ z: https://pl.investing.com/commodities/crude-oil

---

## ğŸ“Š Schemat Bazy Danych

### Tabela: `investing_oil`

```sql
CREATE TABLE investing_oil (
  id BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
  timestamp TIMESTAMP NOT NULL,
  volume TEXT NOT NULL,
  created_at TIMESTAMP DEFAULT NOW()
);
```

**Kolumny:**
- `id` - Unikalny identyfikator (auto-increment)
- `timestamp` - Data i czas pobrania danych (format: YYYY-MM-DD HH:MM:SS)
- `volume` - Wolumen ropy naftowej (tekst, np. "77.626")
- `created_at` - Data utworzenia rekordu w BD

**PrzykÅ‚adowy rekord:**
```json
{
  "id": 1,
  "timestamp": "2025-11-28 16:00:00",
  "volume": "77.626",
  "created_at": "2025-11-28 15:59:59"
}
```

---

## ğŸ”„ Flow Danych

```
1. Railway scheduler â†’ job() uruchamiany o :00 i :30 kaÅ¼dej godziny
        â†“
2. scrape_investing_data() tworzy dict z timestamp + volume
        â†“
3. save_to_csv(data) â†’ zapisuje do investing_oil.csv
        â†“
4. send_to_webhook(data) â†’ wysyÅ‚a JSON do Supabase API
        â†“
5. Supabase INSERT â†’ nowy rekord w tabeli investing_oil
```

---

## ğŸ“ Funkcje GÅ‚Ã³wne

### `scrape_investing_data()`
**Cel:** Pobiera dane i przygotowuje payload do wysÅ‚ania

**Parametry:** Brak

**Zwraca:** Nic (zapisuje dane poprzez `save_to_csv()` i `send_to_webhook()`)

**Pseudo-kod:**
```python
1. Print info o scrapowaniu
2. UtwÃ³rz dict: {"timestamp": current_time, "volume": MOCK_VOLUME}
3. Zapisz do CSV
4. WyÅ›lij do Supabase
5. ObsÅ‚uÅ¼ bÅ‚Ä™dy
```

---

### `save_to_csv(data)`
**Cel:** Zapisuje dane do lokalnego pliku CSV

**Parametry:**
- `data` (dict) - {"timestamp": str, "volume": str}

**Zwraca:** Nic

**Plik:** `investing_oil.csv`

**Format CSV:**
```
timestamp,volume
2025-11-28 15:49:38,77.626
2025-11-28 16:00:00,77.626
```

---

### `send_to_webhook(data)`
**Cel:** WysyÅ‚a dane do Supabase za poÅ›rednictwem REST API

**Parametry:**
- `data` (dict) - {"timestamp": str, "volume": str}

**Zwraca:** Nic

**Endpoint:** `{SUPABASE_URL}/rest/v1/investing_oil`

**NagÅ‚Ã³wki HTTP:**
```
apikey: {SUPABASE_KEY}
Content-Type: application/json
Prefer: return=minimal
```

**Payload JSON:**
```json
{
  "timestamp": "2025-11-28 16:00:00",
  "volume": "77.626"
}
```

**Odpowiedzi:**
- `201` âœ… - Sukces, dane zapisane
- `401` âŒ - BÅ‚Ä…d autoryzacji (zÅ‚e API key)
- `400` âŒ - BÅ‚Ä…d schematu (zÅ‚e pole w payload)

---

### `job()`
**Cel:** Wrapper do uruchomienia scrapera z schedulera

**Parametry:** Brak

**Zwraca:** Nic

**Uruchamiane przez:**
```python
schedule.every().hour.at(":00").do(job)
schedule.every().hour.at(":30").do(job)
```

---

## ğŸ” BezpieczeÅ„stwo

### Row Level Security (RLS) w Supabase

```sql
ALTER TABLE investing_oil ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Allow public inserts" ON investing_oil
  FOR INSERT
  WITH CHECK (true);
```

**Co to robi:**
- Zezwala na INSERT z publicznego API
- Bezpieczna autoryzacja poprzez `apikey` w nagÅ‚Ã³wku
- Nie zawiera wraÅ¼liwych danych

---

## ğŸš€ Deployment & Scheduling

### Railway Configuration

**Plik: `Procfile`**
```
worker: python scraper.py
```

**Co to robi:**
- Railway czyta ten plik
- Uruchamia `python scraper.py` na starcie
- Utrzymuje proces 24/7

### Harmonogram Wykonania

| Czas | Akcja |
|------|-------|
| 00:00 | Scraper uruchamia siÄ™ |
| XX:00 | ğŸ”„ Pobiera dane (job #1) |
| XX:30 | ğŸ”„ Pobiera dane (job #2) |
| KaÅ¼dy 60 sekund | Sprawdza czy wykonaÄ‡ zaplanowane joby |

**PrzykÅ‚ad dzisiaj (28 XI 2025):**
- 15:00 - scraper zbiera dane
- 15:30 - scraper zbiera dane
- 16:00 - scraper zbiera dane
- 16:30 - scraper zbiera dane
- ...i tak co 30 minut

---

## ğŸ“‹ Logi i Debugowanie

### Logi Railway

DostÄ™pne w: Railway Dashboard â†’ Logs

**PrzykÅ‚adowe logi:**
```
ğŸš€ SCRAPER INVESTING.COM URUCHOMIONY!
   Start: 2025-11-28 15:49:38
   Å¹rÃ³dÅ‚o: https://pl.investing.com/commodities/crude-oil
   Zbieranie: o rÃ³wnych godzinach (:00 i :30)
   Tryb: MOCK (dane rÄ™cznie aktualizowane)
==================================================
ğŸ”„ Scrapowanie Investing.com (2025-11-28 15:49:38)...
  ğŸ“Š Wolumen: 77.626 (dane mock)
--------------------------------------------------
âœ… Dane zapisane do investing_oil.csv
âœ… Dane wysÅ‚ane do Supabase: 201
```

### MoÅ¼liwe BÅ‚Ä™dy

| BÅ‚Ä…d | Przyczyna | RozwiÄ…zanie |
|------|-----------|------------|
| `âŒ BÅ‚Ä…d Supabase: HTTPSConnectionPool` | BÅ‚Ä™dny URL Supabase | SprawdÅº SUPABASE_URL w Railway Variables |
| `âŒ BÅ‚Ä…d Supabase: No API key found` | BÅ‚Ä™dny format nagÅ‚Ã³wka | UÅ¼ywaj `apikey` zamiast `Authorization` |
| `âš ï¸ Supabase zwrÃ³ciÅ‚: 401` | ZÅ‚e API key | Regeneruj anon key w Supabase Settings |
| `âš ï¸ Supabase zwrÃ³ciÅ‚: 400` | BÅ‚Ä™dne pole w payload | SprawdziÄ‡ czy payload ma `volume` zamiast `est_volume` |
| `âŒ BÅ‚Ä…d przy zapisywaniu` | Permissions do pliku CSV | Na Railway sprawdÅº storage |

---

## ğŸ”„ Aktualizacja Danych

### RÄ™czna Zmiana Wolumenu

1. OtwÃ³rz `scraper.py` na GitHub
2. Edytuj liniÄ™:
```python
MOCK_VOLUME = "77.626"  # â† ZmieÅ„ tÄ™ wartoÅ›Ä‡
```
3. Kliknij "Commit changes"
4. Railway automatycznie redeploy'uje scraper
5. NastÄ™pny job bÄ™dzie zbieraÄ‡ nowÄ… wartoÅ›Ä‡

### Pobieranie Aktualnego Wolumenu

1. WejdÅº na https://pl.investing.com/commodities/crude-oil
2. Szukaj pola **"Wolumen"**
3. Skopiuj wartoÅ›Ä‡ (np. 77.626)
4. Wklej do `MOCK_VOLUME`

---

## ğŸ“Š WyÅ›wietlanie Danych na Stronie

### Query do Supabase

```javascript
// Pobierz ostatnie 10 rekordÃ³w
const response = await fetch(
  '{SUPABASE_URL}/rest/v1/investing_oil?order=created_at.desc&limit=10',
  {
    headers: {
      'apikey': '{SUPABASE_KEY}',
      'Content-Type': 'application/json'
    }
  }
);

const data = await response.json();
console.log(data);
```

â„¹ï¸ ZastÄ…p `{SUPABASE_URL}` i `{SUPABASE_KEY}` swoimi zmiennymi Å›rodowiskowymi.

### API Endpoint

```
GET https://xqlvexlvvxpkolqrcoxd.supabase.co/rest/v1/investing_oil
```

**Query Parameters:**
- `order=created_at.desc` - Sortuj po dacie malejÄ…co
- `limit=10` - Pobierz ostatnie 10 rekordÃ³w
- `select=timestamp,volume` - Pobierz tylko te kolumny

---

## ğŸ¯ Przypadki UÅ¼ycia

### Monitorowanie Wolumenu w Realtime
```python
# Na stronie pokazuj ostatniÄ… wartoÅ›Ä‡
SELECT volume FROM investing_oil ORDER BY created_at DESC LIMIT 1
```

### Analiza Historyczna
```python
# PokaÅ¼aj wszystkie dane z ostatnich 24 godzin
SELECT * FROM investing_oil 
WHERE created_at > NOW() - INTERVAL '24 hours'
ORDER BY created_at ASC
```

### Export Danych
```python
# Pobierz CSV z historyÄ…
wget https://raw.githubusercontent.com/oprlab/cme_scrap/main/investing_oil.csv
```

---

## ğŸ”§ Utrzymanie i Skalowanie

### Zmiana Harmonogramu

ZmieÅ„ z co 30 minut na inny interwaÅ‚:

```python
# Co 1 godzinÄ™
schedule.every(1).hour.do(job)

# Co 15 minut
schedule.every(15).minutes.do(job)

# Codziennie o 9:00
schedule.every().day.at("09:00").do(job)

# Co poniedziaÅ‚ek o 10:00
schedule.every().monday.at("10:00").do(job)
```

### Dodanie Nowego Pola Danych

1. Edytuj `scraper.py`:
```python
data = {
    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "volume": MOCK_VOLUME,
    "price": MOCK_PRICE,  # â† Nowe pole
}
```

2. Dodaj kolumnÄ™ w Supabase:
```sql
ALTER TABLE investing_oil ADD COLUMN price TEXT;
```

3. Aktualizuj CSV fieldnames:
```python
writer = csv.DictWriter(f, fieldnames=["timestamp", "volume", "price"])
```

---

## ğŸ“š Przydatne Linki

- **Investing.com Oil:** https://pl.investing.com/commodities/crude-oil
- **Supabase Dashboard:** https://supabase.com
- **Railway Dashboard:** https://railway.app
- **GitHub Repository:** https://github.com/oprlab/cme_scrap
- **Schedule Library Docs:** https://schedule.readthedocs.io/

---

## â“ FAQ

**P: Czy mogÄ™ zmieniÄ‡ ÅºrÃ³dÅ‚o danych z Investing.com na inne?**
A: Tak, zmieÅ„ `MOCK_VOLUME` na dane z innego ÅºrÃ³dÅ‚a i zmieÅ„ URL w komentarzu.

**P: Jak dÅ‚ugo bÄ™dÄ… przechowywane dane w Supabase?**
A: Bezterminowo, chyba Å¼e rÄ™cznie usuniesz. Supabase ma lÃ­mity storage w planie free.

**P: Czy scraper zahamuje siÄ™ jeÅ›li Railway bÄ™dzie offline?**
A: Nie, ale bÄ™dzie przeskakiwaÄ‡ joby. Po powrocie w line, weÅºmie siÄ™ do roboty.

**P: Czy mogÄ™ wyÅ›wietliÄ‡ dane na wielu stronach?**
A: Tak, kaÅ¼da strona moÅ¼e query'owaÄ‡ ten sam Supabase endpoint.

**P: Czy dane sÄ… szyfrowane?**
A: Tak, Supabase uÅ¼ywa SSL/TLS do transmisji. Dane w BD to PostgreSQL default encryption.

---

## ğŸ“ Architektura Edukacyjna

To projekt demonstracyjny pokazujÄ…cy:
- âœ… Cloud hosting (Railway)
- âœ… Job scheduling (Schedule library)
- âœ… REST API integration (Supabase)
- âœ… Data persistence (CSV + Database)
- âœ… CI/CD deployment (GitHub â†’ Railway)
- âœ… Error handling & logging
- âœ… Mock data pattern (dla ograniczeÅ„ Å›rodowiska)

---

**Ostatnia aktualizacja:** 28.11.2025
**Status:** âœ… Produkcyjny (dziaÅ‚ajÄ…cy 24/7)
